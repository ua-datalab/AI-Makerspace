{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d716f5fb",
   "metadata": {},
   "source": [
    "# AI Makerspace Session 3\n",
    "## Title: Running OLLAMA on HPC\n",
    "_Hosted by:_ Mithun Paul\n",
    "_Date:_ 10/18/24\n",
    "\n",
    "\n",
    "steps\n",
    "- Run Ollama on laptop\n",
    "- how to connect to HPC\n",
    "- how to install ollama on HPC\n",
    "\n",
    "- Run Ollama on Laptop\n",
    "# # What is [OLLAMA](https://github.com/ua-datalab/Generative-AI/wiki/Running-LLM-Locally:-Ollama)\n",
    "\n",
    "(Omni-Layer Learning Language Acquisition Model)\n",
    "\n",
    "### Connecting to HPC\n",
    "- [OOD](https://ood.hpc.arizona.edu/pun/sys/dashboard)\n",
    "- SSH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db90c7cc-4307-4aa2-b5c4-74583b4de38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?25l\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      "Use Ctrl + d or /bye to exit.\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m"
     ]
    }
   ],
   "source": [
    "#after downloading in OSX, move to applications folder, double click to start, then in a fresh terminal type\n",
    "!ollama run llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c4aa6-14f0-4def-8c55-1053426f1874",
   "metadata": {},
   "source": [
    "### Connecting to HPC\n",
    "- [OOD](https://ood.hpc.arizona.edu/pun/sys/dashboard)\n",
    "- SSH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482928db-5ba8-4ae9-952d-fcf7147023de",
   "metadata": {},
   "source": [
    "# SSH commands to connect to HPC(refer: [docs](https://hpcdocs.hpc.arizona.edu/quick_start/logging_in/#system-access)\n",
    "- ssh mithunpaul@hpc.arizona.edu\n",
    "  Connects to Login node\n",
    "  To connect to Bastion hosts (wentletrap and junonia)type\n",
    "- `shell`\n",
    "switch to elGato\n",
    "- elgato\n",
    "- tmux\n",
    "\n",
    "COmmands to install [Ollama](https://github.com/ollama/ollama/blob/main/docs/linux.md#manual-install)\n",
    "\n",
    "- curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\n",
    "- tar -zxvf ollama-linux-amd64.tgz./\n",
    "-  ./bin/ollama serve\n",
    "\n",
    "  Switch out from that tmux window, or connect through another sessionto HPC\n",
    "  - from the other tmux window run\n",
    "  -  ./bin/ollama run llama3.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
